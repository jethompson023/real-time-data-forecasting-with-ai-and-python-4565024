{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Real-time Feature Store\n",
    "\n",
    "The following pipeline will listen to incoming messages, apply a window function, create our features and store them to a new topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-10-07 13:21:46,908] [INFO] [quixstreams] : Topics required for this application: \"feature_store\", \"incoming_data\"\n",
      "[2024-10-07 13:21:46,913] [INFO] [quixstreams] : Creating a new topic \"feature_store\" with config: \"{'num_partitions': 1, 'replication_factor': 1, 'extra_config': {}}\"\n",
      "[2024-10-07 13:21:47,917] [INFO] [quixstreams] : Topic \"feature_store\" has been created\n",
      "[2024-10-07 13:21:47,918] [INFO] [quixstreams] : Validating Kafka topics exist and are configured correctly...\n",
      "[2024-10-07 13:21:47,947] [INFO] [quixstreams] : Kafka topics validation complete\n",
      "[2024-10-07 13:21:47,957] [INFO] [quixstreams] : Starting the Application with the config: broker_address=\"{'bootstrap.servers': 'localhost:9092'}\" consumer_group=\"example\" auto_offset_reset=\"latest\" commit_interval=5.0s commit_every=0 processing_guarantee=\"at-least-once\"\n",
      "[2024-10-07 13:21:47,957] [INFO] [quixstreams] : Topics required for this application: \"feature_store\", \"incoming_data\", \"changelog__example--incoming_data--default\"\n",
      "[2024-10-07 13:21:47,968] [INFO] [quixstreams] : Creating a new topic \"changelog__example--incoming_data--default\" with config: \"{'num_partitions': 1, 'replication_factor': 1, 'extra_config': {'retention.ms': '604800000', 'retention.bytes': '-1', 'cleanup.policy': 'compact'}}\"\n",
      "[2024-10-07 13:21:48,976] [INFO] [quixstreams] : Topic \"changelog__example--incoming_data--default\" has been created\n",
      "[2024-10-07 13:21:48,977] [INFO] [quixstreams] : Validating Kafka topics exist and are configured correctly...\n",
      "[2024-10-07 13:21:48,986] [INFO] [quixstreams] : Kafka topics validation complete\n",
      "[2024-10-07 13:21:48,987] [INFO] [quixstreams] : Initializing state directory at \"/workspaces/real-time-data-forecasting-with-ai-and-python-4565024/real-time/state/example\"\n",
      "[2024-10-07 13:21:48,990] [INFO] [quixstreams] : Waiting for incoming messages\n",
      "[2024-10-07 13:23:04,614] [INFO] [quixstreams] : Stop processing of StreamingDataFrame\n"
     ]
    }
   ],
   "source": [
    "from quixstreams import Application\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "def parse_period(period_str):\n",
    "    # Streamlined date formats for known input formats\n",
    "    date_formats = ['%Y-%m-%d %H:%M:%S', '%Y-%m-%dT%H:%M:%S', '%Y-%m-%dT%H', '%Y-%m-%d %H:%M']\n",
    "    for date_format in date_formats:\n",
    "        try:\n",
    "            return datetime.strptime(period_str, date_format)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    print(f\"Failed to parse date: {period_str}\")\n",
    "    return None\n",
    "\n",
    "def calculate_lags(window):\n",
    "    # Convert window to a DataFrame\n",
    "    df = pd.DataFrame({'value': window})\n",
    "    \n",
    "    features = {\n",
    "        'lag_1': float(df['value'].shift(1).iloc[-1]) if len(df) > 1 else None,\n",
    "        'lag_2': float(df['value'].shift(2).iloc[-1]) if len(df) > 2 else None,\n",
    "        'lag_6': float(df['value'].shift(6).iloc[-1]) if len(df) > 6 else None,\n",
    "        'lag_12': float(df['value'].shift(12).iloc[-1]) if len(df) > 12 else None,\n",
    "        'lag_24': float(df['value'].shift(24).iloc[-1]) if len(df) > 24 else None,\n",
    "        'rolling_mean_7': float(df['value'].rolling(window=7).mean().iloc[-1]) if len(df) >= 7 else None,\n",
    "        'rolling_std_7': float(df['value'].rolling(window=7).std().iloc[-1]) if len(df) >= 7 else None,\n",
    "    }\n",
    "    \n",
    "    return features\n",
    "\n",
    "def feature_pipeline_online(value, state, producer, feature_store_topic):\n",
    "    period_dt = parse_period(value['Period'])\n",
    "    \n",
    "    if period_dt is None:\n",
    "        print(\"Error parsing the period date.\")\n",
    "        return\n",
    "    \n",
    "    feature_record_id = period_dt.strftime('%Y-%m-%d %H')\n",
    "    \n",
    "    if value['Type'] == 'energy':\n",
    "        window = state.get('energy_window', [])\n",
    "        window.append(value['Value'])        \n",
    "        if len(window) > 25:\n",
    "            window.pop(0)\n",
    "        \n",
    "        state.set('energy_window', window)\n",
    "\n",
    "        features = calculate_lags(window)\n",
    "        features.update({'hour': period_dt.hour,\n",
    "            'day_of_week': period_dt.weekday(),\n",
    "            'month': period_dt.month})\n",
    "        \n",
    "        message = {\n",
    "            \"id\": feature_record_id,\n",
    "            **features\n",
    "        }\n",
    "\n",
    "        producer.produce(topic=feature_store_topic.name, key=feature_record_id, value=json.dumps(message))\n",
    "    \n",
    "    elif value['Type'] == 'temperature':\n",
    "        message = {\n",
    "            'id': feature_record_id,\n",
    "            'temperature_forecast': value['Value']\n",
    "        }\n",
    "        producer.produce(topic=feature_store_topic.name, key=feature_record_id, value=json.dumps(message))\n",
    "\n",
    "app = Application(broker_address='localhost:9092', consumer_group='example')\n",
    "\n",
    "# Define a topic for real-time feature storage with JSON serialization\n",
    "feature_store_topic = app.topic(name='feature_store', value_serializer='json')\n",
    "\n",
    "# Combined data topic setup\n",
    "incoming_data_topic = app.topic(name='incoming_data', value_deserializer='json')\n",
    "\n",
    "# Producer for sending feature data\n",
    "with app.get_producer() as producer:\n",
    "    # Simulate or integrate data handling and processing\n",
    "    data_df = app.dataframe(topic=incoming_data_topic).update(\n",
    "        lambda value, state: feature_pipeline_online(value, state, producer, feature_store_topic), \n",
    "        stateful=True\n",
    "    )\n",
    "    app.run(data_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
